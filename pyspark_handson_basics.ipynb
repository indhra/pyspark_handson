{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0182e2a8",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3a716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f30e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/05 12:57:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mac:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkBasics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x13811ceb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 64029)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/indhra/Machine_learning/pyspark_handson/.venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/indhra/Machine_learning/pyspark_handson/.venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/indhra/Machine_learning/pyspark_handson/.venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/indhra/Machine_learning/pyspark_handson/.venv/lib/python3.10/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkBasics\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea679bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x13811ceb0>\n"
     ]
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfb5ea",
   "metadata": {},
   "source": [
    "# creating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1231c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 25, 'Engineer'), ('Bob', 30, 'Manager'), ('Charlie', 35, 'Analyst')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1: Simple list to DataFrame\n",
    "data = [(\"Alice\", 25, \"Engineer\"),\n",
    "        (\"Bob\", 30, \"Manager\"), \n",
    "        (\"Charlie\", 35, \"Analyst\")]\n",
    "\n",
    "columns = [\"name\", \"age\", \"role\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd511cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|     _1| _2|      _3|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|Engineer|\n",
      "|    Bob| 30| Manager|\n",
      "|Charlie| 35| Analyst|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data\n",
    "                           )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ed2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af218300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   name|age|    role|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|Engineer|\n",
      "|    Bob| 30| Manager|\n",
      "|Charlie| 35| Analyst|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, \n",
    "                           columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33740107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7da9cfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "512d5043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'role']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74372e57",
   "metadata": {},
   "source": [
    "# explicit data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8154a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('role', StringType(), True)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, StructField,StructType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\",IntegerType(), True),\n",
    "    StructField(\"role\",StringType(), True)\n",
    "])\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66598213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   name|age|    role|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|Engineer|\n",
      "|    Bob| 30| Manager|\n",
      "|Charlie| 35| Analyst|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_type = spark.createDataFrame(data, schema)\n",
    "df_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23001f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_type.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3fd56",
   "metadata": {},
   "source": [
    "# selecting, filtering, sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96b02545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name','age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea392dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+\n",
      "|   name|age|   role|\n",
      "+-------+---+-------+\n",
      "|Charlie| 35|Analyst|\n",
      "+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age > 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7026d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------+\n",
      "|name|age|   role|\n",
      "+----+---+-------+\n",
      "| Bob| 30|Manager|\n",
      "+----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.role =='Manager').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41a5b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   name|age|    role|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|Engineer|\n",
      "|    Bob| 30| Manager|\n",
      "|Charlie| 35| Analyst|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a621652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   name|age|    role|\n",
      "+-------+---+--------+\n",
      "|Charlie| 35| Analyst|\n",
      "|    Bob| 30| Manager|\n",
      "|  Alice| 25|Engineer|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.age.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e7e6219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   name|age|    role|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|Engineer|\n",
      "|    Bob| 30| Manager|\n",
      "|Charlie| 35| Analyst|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.age.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67509fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   name|age|    role|\n",
      "+-------+---+--------+\n",
      "|Charlie| 35| Analyst|\n",
      "|  Alice| 25|Engineer|\n",
      "|    Bob| 30| Manager|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('role').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a544a",
   "metadata": {},
   "source": [
    "# column operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d977e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db450145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|   name|age_plus_five|\n",
      "+-------+-------------+\n",
      "|  Alice|           30|\n",
      "|    Bob|           35|\n",
      "|Charlie|           40|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', (col(\"age\") + 5).alias('age_plus_five')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a81b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|   name|age_plus_fiveish|\n",
      "+-------+----------------+\n",
      "|  Alice|            30.2|\n",
      "|    Bob|            35.2|\n",
      "|Charlie|            40.2|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1= df.select('name', (col('age')+5.2).alias('age_plus_fiveish'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75446ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age_plus_fiveish: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab2ae4",
   "metadata": {},
   "source": [
    "# grouping and aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4f22901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'Sales', 'NY', 90000, 34, 10000),\n",
       " ('Michael', 'Sales', 'NY', 86000, 56, 20000),\n",
       " ('Robert', 'Sales', 'CA', 81000, 30, 23000),\n",
       " ('Maria', 'Finance', 'CA', 90000, 24, 23000),\n",
       " ('Raman', 'Finance', 'CA', 99000, 40, 24000),\n",
       " ('Scott', 'Finance', 'NY', 83000, 36, 19000),\n",
       " ('Jen', 'Finance', 'NY', 79000, 53, 15000),\n",
       " ('Jeff', 'Marketing', 'CA', 80000, 25, 18000),\n",
       " ('Kumar', 'Marketing', 'NY', 91000, 50, 21000)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "    (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "    (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "    (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "    (\"Raman\", \"Finance\", \"CA\", 99000, 40, 24000),\n",
    "    (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n",
    "    (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "    (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "    (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "]\n",
    "columns = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e76b3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6df7a588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+\n",
      "|state|sum(salary)|max(age)|\n",
      "+-----+-----------+--------+\n",
      "|   NY|     429000|      56|\n",
      "|   CA|     350000|      40|\n",
      "+-----+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max\n",
    "\n",
    "df.groupBy('state').agg(sum('salary'),max('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a0f3463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------+\n",
      "|department|highest_bonus|lowest_bonus|\n",
      "+----------+-------------+------------+\n",
      "|     Sales|        23000|       10000|\n",
      "|   Finance|        24000|       15000|\n",
      "| Marketing|        21000|       18000|\n",
      "+----------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('department').agg(max('bonus').alias('highest_bonus'), min('bonus').alias('lowest_bonus')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58c5e11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   NY|    5|\n",
      "|   CA|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\n",
    "    \"state\"\n",
    ").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d0dec",
   "metadata": {},
   "source": [
    "# window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a3b5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70412366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.classic.window.WindowSpec object at 0x1381bb610>\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('department').orderBy('Salary')\n",
    "print(window_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05d8e88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+-----------+\n",
      "|employee_name|department|state|salary|age|bonus|salary_rank|\n",
      "+-------------+----------+-----+------+---+-----+-----------+\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|          1|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|          2|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|          3|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|          4|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|          1|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|          2|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|          1|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|          2|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|          3|\n",
      "+-------------+----------+-----+------+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('salary_rank', rank().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79406f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+-----------+\n",
      "|employee_name|department|state|salary|age|bonus|aged_person|\n",
      "+-------------+----------+-----+------+---+-----+-----------+\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|          1|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|          2|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|          3|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|          4|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|          1|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|          2|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|          1|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|          2|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|          3|\n",
      "+-------------+----------+-----+------+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy('department').orderBy(df.age.desc())\n",
    "df.withColumn('aged_person',rank().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783857a",
   "metadata": {},
   "source": [
    "# joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90d0a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_data = [(\"Finance\", 10), (\"Marketing\", 20), (\"Sales\", 30)]\n",
    "dept_columns = [\"dept_name\", \"dept_id\"]\n",
    "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
    "\n",
    "emp_data = [(1,\"John\",10), (2,\"Maria\",20), (3,\"David\",10)]\n",
    "emp_columns = [\"emp_id\", \"name\", \"emp_dept_id\"]\n",
    "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a18d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+\n",
      "|emp_id| name|emp_dept_id|\n",
      "+------+-----+-----------+\n",
      "|     1| John|         10|\n",
      "|     2|Maria|         20|\n",
      "|     3|David|         10|\n",
      "+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7302158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+---------+-------+\n",
      "|emp_id| name|emp_dept_id|dept_name|dept_id|\n",
      "+------+-----+-----------+---------+-------+\n",
      "|     1| John|         10|  Finance|     10|\n",
      "|     3|David|         10|  Finance|     10|\n",
      "|     2|Maria|         20|Marketing|     20|\n",
      "+------+-----+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = emp_df.join(dept_df, dept_df.dept_id == emp_df.emp_dept_id, how='inner')\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "031b1f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+---------+-------+\n",
      "|emp_id| name|emp_dept_id|dept_name|dept_id|\n",
      "+------+-----+-----------+---------+-------+\n",
      "|     1| John|         10|  Finance|     10|\n",
      "|     2|Maria|         20|Marketing|     20|\n",
      "|     3|David|         10|  Finance|     10|\n",
      "+------+-----+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = emp_df.join(dept_df, dept_df.dept_id == emp_df.emp_dept_id, how='left')\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ae77eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+---------+-------+\n",
      "|emp_id| name|emp_dept_id|dept_name|dept_id|\n",
      "+------+-----+-----------+---------+-------+\n",
      "|     3|David|         10|  Finance|     10|\n",
      "|     1| John|         10|  Finance|     10|\n",
      "|     2|Maria|         20|Marketing|     20|\n",
      "|  NULL| NULL|       NULL|    Sales|     30|\n",
      "+------+-----+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = emp_df.join(dept_df, dept_df.dept_id == emp_df.emp_dept_id, how='right')\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69dc103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+---------+-------+\n",
      "|emp_id| name|emp_dept_id|dept_name|dept_id|\n",
      "+------+-----+-----------+---------+-------+\n",
      "|     1| John|         10|  Finance|     10|\n",
      "|     3|David|         10|  Finance|     10|\n",
      "|     2|Maria|         20|Marketing|     20|\n",
      "|  NULL| NULL|       NULL|    Sales|     30|\n",
      "+------+-----+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = emp_df.join(dept_df, dept_df.dept_id == emp_df.emp_dept_id, how='outer')\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f65ef",
   "metadata": {},
   "source": [
    "# caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c656cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd3e4ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[employee_name: string, department: string, state: string, salary: bigint, age: bigint, bonus: bigint]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af413e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "039f8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53a41d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a8e4c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_id: bigint, name: string, emp_dept_id: bigint, dept_name: string, dept_id: bigint]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aab03681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformation - lazy\n",
    "older = df.filter(df.age>30)\n",
    "\n",
    "# action - triggers job\n",
    "older.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5fcdef",
   "metadata": {},
   "source": [
    "# distributed computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a214cd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "572122c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increase partitions  - shuffle\n",
    "\n",
    "df2 = df.repartition(10)\n",
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df46f317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df.coalesce(2)\n",
    "df3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639f262",
   "metadata": {},
   "source": [
    "# read, write data, csv, parquet, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08957520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+-----------+-----------+------+---+----------------+-----------------+----------------+------------+-----+----------+---------+---------------+-------------+---------------------+\n",
      "|employee_id| first_name|last_name| department|  job_title|salary|age|years_experience|performance_score|bonus_percentage|        city|state| hire_date|is_remote|education_level|project_count|customer_satisfaction|\n",
      "+-----------+-----------+---------+-----------+-----------+------+---+----------------+-----------------+----------------+------------+-----+----------+---------+---------------+-------------+---------------------+\n",
      "|          1|      Sarah| Martinez|      Sales|     Senior| 78511| 49|               0|             3.94|            1.32|Philadelphia|   AZ|2021-03-14|    false|       Bachelor|            7|                  7.8|\n",
      "|          2|       John|    Davis|      Legal|Coordinator| 57346| 43|              25|             4.01|            2.26|      Dallas|   TX|2023-10-28|     true|       Bachelor|            3|                  4.5|\n",
      "|          3|      James|   Miller|    Finance|    Analyst| 90732| 49|              18|             4.06|            7.17| San Antonio|   CA|2022-01-16|    false|    High School|            3|                  8.5|\n",
      "|          4|       Lisa| Gonzalez|  Marketing| Specialist| 92778| 49|              21|             1.94|            1.62| San Antonio|   PA|2023-07-02|    false|    High School|            1|                  9.6|\n",
      "|          5|       Lisa|    Brown|Engineering|    Manager| 30000| 51|              22|             4.37|           14.55| Los Angeles|   AZ|2020-10-17|     true|       Bachelor|            1|                  1.4|\n",
      "|          6|      David| Gonzalez|Engineering|    Manager|161093| 53|               7|              1.1|            4.01|     Houston|   IL|2022-07-01|     true|       Bachelor|            3|                  6.4|\n",
      "|          7|      Sarah|    Smith|Engineering|    Analyst| 42914| 64|              22|             2.73|           12.97|      Dallas|   IL|2023-11-04|     true|         Master|            0|                  8.6|\n",
      "|          8|    Barbara|   Wilson| Operations|    Analyst| 92922| 38|              20|             1.73|            3.01|    San Jose|   TX|2022-07-14|     true|         Master|            2|                  1.4|\n",
      "|          9|    Michael|   Miller|    Finance| Specialist| 63684| 43|              37|             3.85|            4.94|   San Diego|   PA|2023-08-14|    false|            PhD|            3|                  2.0|\n",
      "|         10|Christopher|  Johnson|    Finance|    Analyst| 83262| 25|              13|             4.86|           15.44|Philadelphia|   TX|2022-08-30|    false|    High School|            1|                  3.1|\n",
      "|         11|   Patricia|   Garcia|Engineering|    Analyst| 59388| 54|              12|             3.06|           14.91|     Chicago|   NY|2023-07-28|    false|         Master|            3|                  8.3|\n",
      "|         12|       Jane|    Lopez|      Legal|    Manager|111839| 63|              32|             2.95|           11.08|    San Jose|   IL|2021-05-04|    false|    High School|            2|                  1.6|\n",
      "|         13|       John|    Moore|         HR|    Analyst| 39147| 34|              28|             3.18|            1.78| San Antonio|   CA|2020-09-04|    false|         Master|            2|                  7.7|\n",
      "|         14|    Michael|    Davis|Engineering|       Lead|100843| 31|               5|             2.44|            6.54|    New York|   IL|2022-07-07|    false|       Bachelor|            1|                  5.1|\n",
      "|         15|     Robert| Gonzalez|Engineering|    Manager| 30000| 57|              30|             3.58|            6.67|    New York|   IL|2022-07-30|    false|         Master|            4|                  9.1|\n",
      "|         16|       Lisa| Gonzalez|Engineering|     Senior| 77419| 56|              31|             3.51|            1.53|Philadelphia|   PA|2022-11-01|     true|       Bachelor|            2|                  5.5|\n",
      "|         17|     Thomas|    Moore|      Sales|Coordinator| 40329| 55|              16|             2.29|            0.93|Philadelphia|   CA|2021-12-16|     true|       Bachelor|            7|                  9.2|\n",
      "|         18|  Elizabeth|    Jones|  Marketing| Specialist| 92829| 51|              25|             2.87|            0.07| San Antonio|   IL|2023-07-25|    false|       Bachelor|            1|                  7.4|\n",
      "|         19|       John| Martinez|      Sales|    Analyst| 83552| 54|              32|             3.28|           18.93|     Houston|   AZ|2023-05-27|    false|    High School|            3|                  1.4|\n",
      "|         20|    Barbara|  Johnson|      Sales|    Analyst| 88126| 33|               5|             2.48|           17.89|Philadelphia|   AZ|2022-08-11|     true|         Master|            2|                  7.6|\n",
      "+-----------+-----------+---------+-----------+-----------+------+---+----------------+-----------------+----------------+------------+-----+----------+---------+---------------+-------------+---------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.csv('./employee_data.csv', header=True, inferSchema=True)\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23f997a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix for Parquet timestamp type compatibility\n",
    "# spark.conf.set(\"spark.sql.parquet.int96AsTimestamp\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"LEGACY\")\n",
    "# spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"LEGACY\")\n",
    "# spark.conf.set(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MILLIS\")\n",
    "\n",
    "# df_parq = spark.read.option(\"mergeSchema\", \"true\").option(\"timeZone\", \"UTC\").parquet('./employee_data.parquet')\n",
    "# # Display the contents of the Parquet DataFrame\n",
    "# df_parq.show()\n",
    "# # Print the schema for clarity and learning\n",
    "# df_parq.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98300d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_json = spark.read.json('./employee_data.json')\n",
    "# df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39f27c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.write.option('header',True).csv('save1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "664f356b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+-------+---------+---------+---------+\n",
      "|department|state|total_salary|avg_age|min_bonus|max_bonus|emp_count|\n",
      "+----------+-----+------------+-------+---------+---------+---------+\n",
      "|     Sales|   NY|      176000|   45.0|    10000|    20000|        2|\n",
      "|     Sales|   CA|       81000|   30.0|    23000|    23000|        1|\n",
      "|   Finance|   CA|      189000|   32.0|    23000|    24000|        2|\n",
      "|   Finance|   NY|      162000|   44.5|    15000|    19000|        2|\n",
      "| Marketing|   NY|       91000|   50.0|    21000|    21000|        1|\n",
      "| Marketing|   CA|       80000|   25.0|    18000|    18000|        1|\n",
      "+----------+-----+------------+-------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max, count\n",
    "\n",
    "df.groupBy(\"department\", \"state\").agg(\n",
    "    sum(\"salary\").alias(\"total_salary\"),\n",
    "    avg(\"age\").alias(\"avg_age\"),\n",
    "    min(\"bonus\").alias(\"min_bonus\"),\n",
    "    max(\"bonus\").alias(\"max_bonus\"),\n",
    "    count(\"*\").alias(\"emp_count\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a99a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when \n",
    "\n",
    "# Add a new column 'age_group' based on age ranges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f520138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+---------+\n",
      "|employee_name|department|state|salary|age|bonus|age_group|\n",
      "+-------------+----------+-----+------+---+-----+---------+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|   middle|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|   middle|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|    young|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|    young|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|   middle|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|   middle|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|   middle|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|    young|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|   middle|\n",
      "+-------------+----------+-----+------+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(df.age<=30, 'young')\n",
    "    .when(df.age > 60, 'senior')\n",
    "    .otherwise('middle')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea8f4d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', ['Java', 'Scala']),\n",
       " ('Michael', ['Spark', 'Java']),\n",
       " ('Robert', ['CSharp']),\n",
       " ('Washington', None),\n",
       " ('Jefferson', ['1', '2'])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "data = [\n",
    "    (\"James\", [\"Java\", \"Scala\"]),\n",
    "    (\"Michael\", [\"Spark\", \"Java\"]),\n",
    "    (\"Robert\", [\"CSharp\"]),\n",
    "    (\"Washington\", None),\n",
    "    (\"Jefferson\", [\"1\", \"2\"])\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7cf6d056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|      name|       skills|\n",
      "+----------+-------------+\n",
      "|     James|[Java, Scala]|\n",
      "|   Michael|[Spark, Java]|\n",
      "|    Robert|     [CSharp]|\n",
      "|Washington|         NULL|\n",
      "| Jefferson|       [1, 2]|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arr = spark.createDataFrame(data, ['name','skills'])\n",
    "df_arr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4168d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|   Robert|CSharp|\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfarr1 = df_arr.select('name', explode(\"skills\"))\n",
    "dfarr1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f35b4cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9881431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+-----------------+\n",
      "|employee_name|department|state|salary|age|bonus|moving_avg_salary|\n",
      "+-------------+----------+-----+------+---+-----+-----------------+\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|          79000.0|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|          81000.0|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|          84000.0|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|90666.66666666667|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|          80000.0|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|          85500.0|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|          81000.0|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|          83500.0|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|85666.66666666667|\n",
      "+-------------+----------+-----+------+---+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(\"salary\").rowsBetween(-2, 0)\n",
    "df.withColumn(\"moving_avg_salary\", avg(\"salary\").over(window_spec)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5e921",
   "metadata": {},
   "source": [
    "# UDF and pandas udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b7601d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+-----------+\n",
      "|employee_name|department|state|salary|age|bonus|bonus_label|\n",
      "+-------------+----------+-----+------+---+-----+-----------+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|        Low|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|       High|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|       High|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|       High|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|       High|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|        Low|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|        Low|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|        Low|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|       High|\n",
      "+-------------+----------+-----+------+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def label_bonus(bonus):\n",
    "    return \"High\" if bonus >= 20000 else \"Low\"\n",
    "\n",
    "bonus_udf = udf(label_bonus, StringType())\n",
    "df.withColumn(\"bonus_label\", bonus_udf(df.bonus)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c79e0abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+---------------+------+\n",
      "|order_id|amount|country|customer_status|  risk|\n",
      "+--------+------+-------+---------------+------+\n",
      "|  IN1001|   950|     US|        regular|normal|\n",
      "|  IN1002|  3500|     IN|            vip|medium|\n",
      "|  IN1003| 12000|     US|        blocked|  high|\n",
      "|  IN1004|  9000|     UK|        regular|normal|\n",
      "|  IN1005|   250|     FR|            vip|normal|\n",
      "+--------+------+-------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"IN1001\", 950, \"US\", \"regular\"),\n",
    "    (\"IN1002\", 3500, \"IN\", \"vip\"),\n",
    "    (\"IN1003\", 12000, \"US\", \"blocked\"),\n",
    "    (\"IN1004\", 9000, \"UK\", \"regular\"),\n",
    "    (\"IN1005\", 250, \"FR\", \"vip\")\n",
    "]\n",
    "columns = [\"order_id\", \"amount\", \"country\", \"customer_status\"]\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UDFMultiColExample\").getOrCreate()\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Advanced UDF using multiple columns\n",
    "def risk_label(amount, country, status):\n",
    "    if status == \"blocked\":\n",
    "        return \"high\"\n",
    "    if amount > 10000:\n",
    "        return \"high\"\n",
    "    if amount > 2000 and country == \"IN\":\n",
    "        return \"medium\"\n",
    "    if (country in [\"US\", \"UK\"]) and status == \"vip\":\n",
    "        return \"low\"\n",
    "    return \"normal\"\n",
    "\n",
    "risk_udf = udf(risk_label, StringType())\n",
    "\n",
    "df = df.withColumn(\"risk\", risk_udf(df.amount, df.country, df.customer_status))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e061a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+---------------+------+-----------+\n",
      "|order_id|amount|country|customer_status|  risk|risk_pandas|\n",
      "+--------+------+-------+---------------+------+-----------+\n",
      "|  IN1001|   950|     US|        regular|normal|     normal|\n",
      "|  IN1002|  3500|     IN|            vip|medium|     medium|\n",
      "|  IN1003| 12000|     US|        blocked|  high|       high|\n",
      "|  IN1004|  9000|     UK|        regular|normal|     normal|\n",
      "|  IN1005|   250|     FR|            vip|normal|     normal|\n",
      "+--------+------+-------+---------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def fast_risk_label(amount: pd.Series, country: pd.Series, status: pd.Series) -> pd.Series:\n",
    "    # pandas magic for batch processing\n",
    "    cond_high = (status == \"blocked\") | (amount > 10000)\n",
    "    cond_medium = (amount > 2000) & (country == \"IN\")\n",
    "    cond_low = (country.isin([\"US\",\"UK\"])) & (status == \"vip\")\n",
    "    return pd.Series(\n",
    "        [\"high\" if h else \"medium\" if m else \"low\" if l else \"normal\"\n",
    "         for h, m, l in zip(cond_high, cond_medium, cond_low)]\n",
    "    )\n",
    "\n",
    "df = df.withColumn(\"risk_pandas\", fast_risk_label(df.amount, df.country, df.customer_status))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff25324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04fa1742",
   "metadata": {},
   "source": [
    "# pivot and unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e1e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|region|month|sales|\n",
      "+------+-----+-----+\n",
      "| North|  Jan|  200|\n",
      "| North|  Feb|  150|\n",
      "| South|  Jan|   50|\n",
      "| South|  Feb|  120|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, first, last\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"North\", \"Jan\", 200),\n",
    "    (\"North\", \"Feb\", 150),\n",
    "    (\"South\", \"Jan\", 50),\n",
    "    (\"South\", \"Feb\", 120)\n",
    "]\n",
    "columns = [\"region\", \"month\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72ea6576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+\n",
      "|region|Feb|Jan|\n",
      "+------+---+---+\n",
      "| South|120| 50|\n",
      "| North|150|200|\n",
      "+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pivoted = df.groupBy(\"region\").pivot(\"month\").agg(sum(\"sales\"))\n",
    "pivoted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d6a17df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- region: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- sales: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f1b4e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+\n",
      "|region|Feb|Jan|\n",
      "+------+---+---+\n",
      "| South|120| 50|\n",
      "| North|150|200|\n",
      "+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first,last\n",
    "\n",
    "# Example: pivot with string values, extracting the first string per group\n",
    "pivoted_str = df.groupBy(\"region\").pivot(\"month\").agg(last(\"sales\"))\n",
    "pivoted_str.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d303d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|region|month|sales|\n",
      "+------+-----+-----+\n",
      "| South|  Jan|   50|\n",
      "| South|  Feb|  120|\n",
      "| North|  Jan|  200|\n",
      "| North|  Feb|  150|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark (3.4+)\n",
    "melted = pivoted.melt(ids=[\"region\"], values=[\"Jan\", \"Feb\"], \n",
    "                      variableColumnName=\"month\", valueColumnName=\"sales\")\n",
    "melted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c9e88cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|skill|\n",
      "+----+-----+\n",
      "|Anna|   AI|\n",
      "|Anna|   ML|\n",
      "|Bala|   DB|\n",
      "|Bala|  Web|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "data = [(\"Anna\", [\"AI\", \"ML\"]), (\"Bala\", [\"DB\", \"Web\"]), (\"Cara\", None)]\n",
    "df2 = spark.createDataFrame(data, [\"name\", \"skills\"])\n",
    "df_exploded = df.select(\"name\", explode(\"skills\").alias(\"skill\"))\n",
    "df_exploded.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "81fde758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+---------+\n",
      "|color|avg_score|n_samples|max_score|\n",
      "+-----+---------+---------+---------+\n",
      "|  Red|    110.0|        2|      120|\n",
      "| Blue|    100.0|        2|      110|\n",
      "+-----+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count, max\n",
    "\n",
    "data = [\n",
    "    (\"Red\", 2019, 100),\n",
    "    (\"Red\", 2020, 120),\n",
    "    (\"Blue\", 2019, 90),\n",
    "    (\"Blue\", 2020, 110)\n",
    "]\n",
    "cols = [\"color\", \"year\", \"score\"]\n",
    "df = spark.createDataFrame(data, cols)\n",
    "\n",
    "agg_df = df.groupBy(\"color\").agg(\n",
    "    avg(\"score\").alias(\"avg_score\"),\n",
    "    count(\"*\").alias(\"n_samples\"),\n",
    "    max(\"score\").alias(\"max_score\")\n",
    ")\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e2e83a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+---------+\n",
      "|color|avg_score|n_samples|max_score|\n",
      "+-----+---------+---------+---------+\n",
      "|  Red|    110.0|        2|      120|\n",
      "+-----+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agg_df.filter(\"avg_score > 100\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c65503",
   "metadata": {},
   "source": [
    "# pyspark.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "49709f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        score\n",
      "animal       \n",
      "cat       7.0\n",
      "dog       4.0\n"
     ]
    }
   ],
   "source": [
    "# Disable ANSI mode for pandas-on-Spark compatibility\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "pdf = ps.DataFrame({\n",
    "    'animal': ['cat', 'dog', 'cat', 'dog'],\n",
    "    'score': [5, 7, 9, 1]\n",
    "})\n",
    "\n",
    "pdf_grouped = pdf.groupby('animal').mean()\n",
    "pdf_grouped = pdf_grouped.sort_values('score', ascending=False)\n",
    "print(pdf_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fcd2560b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  animal  score\n",
       "0    cat      5\n",
       "1    dog      7\n",
       "2    cat      9\n",
       "3    dog      1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.explode('score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9657584c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>score</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>score</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat</td>\n",
       "      <td>score</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>score</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  animal variable  value\n",
       "0    cat    score      5\n",
       "1    dog    score      7\n",
       "2    cat    score      9\n",
       "3    dog    score      1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.melt(id_vars='animal', value_vars=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e4df61bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|key|val1|\n",
      "+---+----+\n",
      "|  A|  10|\n",
      "|  B|  20|\n",
      "+---+----+\n",
      "\n",
      "+---+----+\n",
      "|key|val2|\n",
      "+---+----+\n",
      "|  A|  99|\n",
      "|  C|  88|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left = spark.createDataFrame(\n",
    "    [(\"A\", 10), (\"B\", 20)], [\"key\", \"val1\"]\n",
    ")\n",
    "right = spark.createDataFrame(\n",
    "    [(\"A\", 99), (\"C\", 88)], [\"key\", \"val2\"]\n",
    ")\n",
    "left.show()\n",
    "right.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e3c67b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "|key|val1|val2|\n",
      "+---+----+----+\n",
      "|  A|  10|  99|\n",
      "+---+----+----+\n",
      "\n",
      "+---+----+----+\n",
      "|key|val1|val2|\n",
      "+---+----+----+\n",
      "|  A|  10|  99|\n",
      "|  B|  20|NULL|\n",
      "+---+----+----+\n",
      "\n",
      "+---+----+----+\n",
      "|key|val1|val2|\n",
      "+---+----+----+\n",
      "|  A|  10|  99|\n",
      "|  B|  20|NULL|\n",
      "|  C|NULL|  88|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inner = left.join(right, on=\"key\", how=\"inner\")\n",
    "left_outer = left.join(right, on=\"key\", how=\"left_outer\")\n",
    "full_outer = left.join(right, on=\"key\", how=\"outer\")\n",
    "inner.show()\n",
    "left_outer.show()\n",
    "full_outer.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "acfe888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| name| age|grade|\n",
      "+-----+----+-----+\n",
      "|Alice|  54| NULL|\n",
      "|  Bob|NULL|    A|\n",
      "| Cara|  33|    B|\n",
      "|  Dan|NULL| NULL|\n",
      "+-----+----+-----+\n",
      "\n",
      "+----------+---------+-----------+\n",
      "|name_nulls|age_nulls|grade_nulls|\n",
      "+----------+---------+-----------+\n",
      "|         0|        2|          2|\n",
      "+----------+---------+-----------+\n",
      "\n",
      "+----+---+-----+\n",
      "|name|age|grade|\n",
      "+----+---+-----+\n",
      "|Cara| 33|    B|\n",
      "+----+---+-----+\n",
      "\n",
      "+-----+---+-------+\n",
      "| name|age|  grade|\n",
      "+-----+---+-------+\n",
      "|Alice| 54|Unknown|\n",
      "|  Bob|  0|      A|\n",
      "| Cara| 33|      B|\n",
      "|  Dan|  0|Unknown|\n",
      "+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnull, when, count\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = [(\"Alice\", 54, None), (\"Bob\", None, \"A\"), (\"Cara\", 33, \"B\"), (\"Dan\", None, None)]\n",
    "cols = [\"name\", \"age\", \"grade\"]\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show()\n",
    "\n",
    "# Count nulls per column\n",
    "df.select([count(when(isnull(c), c)).alias(c + \"_nulls\") for c in df.columns]).show()\n",
    "\n",
    "# Drop rows with any nulls\n",
    "df_drop = df.dropna()\n",
    "df_drop.show()\n",
    "\n",
    "# Fill nulls with default values\n",
    "df_fill = df.fillna({\"age\": 0, \"grade\": \"Unknown\"})\n",
    "df_fill.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3a79a26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-----------+\n",
      "| name|age|grade|grade_clean|\n",
      "+-----+---+-----+-----------+\n",
      "|Alice| 54| NULL|       NULL|\n",
      "| Cara| 33|    B|          b|\n",
      "+-----+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, trim\n",
    "\n",
    "result = (\n",
    "    df.fillna({\"age\": 100})\n",
    "      .filter(df.age > 30)\n",
    "      .withColumn(\"grade_clean\", lower(trim(df.grade)))\n",
    ")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4f0153ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+--------+\n",
      "|cat|total| mean|n_unique|\n",
      "+---+-----+-----+--------+\n",
      "|  B|   50| 50.0|       1|\n",
      "|  C|   30| 30.0|       1|\n",
      "|  A|  300|150.0|       2|\n",
      "+---+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, countDistinct\n",
    "\n",
    "data = [(\"A\", 100), (\"A\", 200), (\"B\", 50), (\"B\", None), (\"C\", 30)]\n",
    "df = spark.createDataFrame(data, [\"cat\", \"val\"])\n",
    "\n",
    "aggd = df.groupBy(\"cat\").agg(\n",
    "    sum(\"val\").alias(\"total\"),\n",
    "    avg(\"val\").alias(\"mean\"),\n",
    "    countDistinct(\"val\").alias(\"n_unique\")\n",
    ")\n",
    "aggd.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0da4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
